{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devica2000/StanfordTech16LLM/blob/main/TECH16_LLM_HW1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai"
      ],
      "metadata": {
        "id": "UAsj88npPdRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67fa93ff-7f26-4cb0-b3df-8f8bde59d77f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.35.13-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.35.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to read from the secret key\n",
        "#we later pass this key to the open AI library when we are about to call it\n",
        "from google.colab import userdata\n",
        "open_ai_key = userdata.get('open_ai_key')"
      ],
      "metadata": {
        "id": "HxqAFkVJCTAw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the contents of the open ai library\n",
        "\n",
        "from openai import OpenAI\n",
        "contents = dir(OpenAI)\n",
        "print(len(contents))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_eVViPCLC62J",
        "outputId": "adbf5792-943f-4432-eac2-99a00bb046f7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(api_key = open_ai_key)"
      ],
      "metadata": {
        "id": "2Y1WQERNDbJZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is about compelting text\n",
        "#we give it some text, it tries to predict the next word\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a RAP artist that takes instructions from a human and produces an answer. Be concise in your output. Talk like a RAP artist\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who is the current best player in Argentina soccer team\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpMTTubaDs2b",
        "outputId": "29f0ad45-ac77-4b30-c373-fd8142aa6402"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lionel Messi, no doubt he's the king,\n",
            "On the Argentina team, he's the number one thing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#the model is about compelting text\n",
        "#we give it some text, it tries to predict the next word\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI that takes instructions from a human and produces an answer. Be concise in your output.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who is the current best player in Argentina's soccer team\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "# print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2pLJbv3FSfI",
        "outputId": "3af3f2cc-afdd-43ee-ef18-012ef0cffe05"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lionel Messi.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message):\n",
        "\n",
        "  response = client.chat.completions.create(\n",
        "    model='gpt-3.5-turbo',\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an AI that takes instructions from a human and produces an answer. Be concise in your output.\"},\n",
        "        {\"role\": \"user\", \"content\": \"Who is the current best player in Argentina's soccer team\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "  text_only = response.choices[0].message.content\n",
        "  return text_only"
      ],
      "metadata": {
        "id": "lKWIQ74xGsQ1"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"Who is the G.O.A.T in football?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gpVkB0HKHCHk",
        "outputId": "38ab9f59-8313-4ac8-a457-a3faa0bb4a36"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Lionel Messi.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the content being \"Who is current best player in Argentina soccer team?\", the answer was - LM is currently considered the best player in Argentina's soccer team.\n",
        "\n",
        "Is the question was, \"Who is the current best player in Argentina's soccer team?\", the answer was Lionel Messi and then back again."
      ],
      "metadata": {
        "id": "HRZDkJ2wFlQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ORIGINAL CODE FOLLOWS"
      ],
      "metadata": {
        "id": "M82PU4EwHxBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "\n",
        "client = OpenAI(api_key=open_ai_key)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  # response_format={ \"type\": \"json_object\" },\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"You are an AI that takes instructions from a human and produces an answer. Be concise in your output.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Compose a haiku about students studing large language models in Stanford on a Wednesday night.\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "Ft0dVY1iOLhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c47cd49b-ff34-40fb-d5a3-98d59cf1d2a5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='chatcmpl-9l9g8xPefLhao1TQG3CNfaTtP5ENo', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Students at Stanford,\\nLanguage models they explore,\\nWednesday night studies.', role='assistant', function_call=None, tool_calls=None))], created=1721025648, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=13, prompt_tokens=50, total_tokens=63))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "id": "7YKYlHu_R2Vn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a96da5e-8c28-4c0a-fb64-747d04fa7f79"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Students at Stanford,\n",
            "Language models they explore,\n",
            "Wednesday night studies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT-3.5 model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT-3.5-turbo model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT-3.5 model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an AI that takes instructions from a human and produces an answer. Be concise in your output.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"{message}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only\n"
      ],
      "metadata": {
        "id": "u3c4jVGiSgdl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"What is the tallest building in NYC?\")"
      ],
      "metadata": {
        "id": "ipaLzZkRUsRm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "94cefc27-7176-4f61-a72b-72eefa46db40"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'One World Trade Center.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework starter"
      ],
      "metadata": {
        "id": "jaKXksKrUl5n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "\n",
        "client = OpenAI(api_key=open_ai_key)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Summarize the text into a headline\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "          Meta will reward shareholders with its first-ever dividend and an additional $50bn in share buybacks\n",
        "        as bumper fourth-quarter results sent its shares up by more than 14 per cent in after-hours trading\n",
        "        on Thursday.\n",
        "        \"\"\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "summary = response.choices[0].message.content\n",
        "print(summary)"
      ],
      "metadata": {
        "id": "zvyyGYKYUlUU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6648dc91-c04a-4ba1-8393-3418d9b06873"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta to Issue First Dividend and $50bn in Share Buybacks After Strong Fourth Quarter Results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "\n",
        "client = OpenAI(api_key=open_ai_key)\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo\",\n",
        "  messages=[\n",
        "    {\"role\": \"system\", \"content\": \"Summarize the text. Be concise in your output.\"},\n",
        "    {\"role\": \"user\", \"content\": \"\"\"\n",
        "          Google is considering spending $23 billion to buy Wiz, a cloud cybersecurity startup with partners\n",
        "          that include Amazon and Oracle, reports The Wall Street Journal. At close to twice what it spent\n",
        "          for Motorola Mobility in 2012, it would be the most Google has ever paid for another company.\n",
        "        \"\"\"}\n",
        "  ]\n",
        ")\n",
        "\n",
        "summary = response.choices[0].message.content\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMLuY3S-I3tQ",
        "outputId": "a1824002-18fb-4d69-980a-74c668799f66"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google is potentially looking to acquire Wiz, a cloud cybersecurity startup, for $23 billion, which would be the largest amount the company has spent on acquiring another company.\n"
          ]
        }
      ]
    }
  ]
}