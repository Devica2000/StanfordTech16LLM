{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devica2000/StanfordTech16LLM/blob/main/TECH16_LLM_Lecture2_CW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "UAsj88npPdRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40cc3073-4dac-4fb9-8ffc-ba869626ae18"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.37.0-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.8.2)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.7.4)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.20.1)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 openai-1.37.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "open_ai_key = userdata.get('open_ai_key')\n",
        "client = OpenAI(api_key=open_ai_key)\n"
      ],
      "metadata": {
        "id": "Ft0dVY1iOLhv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt engineering - few shot"
      ],
      "metadata": {
        "id": "0X61OO5zwVZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT-3.5 model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT-3.5-turbo model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT-3.5 model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        temperature=0.3,\n",
        "        # response_format={ \"type\": \"json_object\" },\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"Below are examples of text messages and their classifications. After studying these examples, please classify the new text message at the end.\n",
        "\n",
        "              Example 1:\n",
        "\n",
        "              Text: \"Can you send me the files by tomorrow? It's not urgent, but I'd like to review them.\"\n",
        "              Classification: Non-Urgent\n",
        "              Example 2:\n",
        "\n",
        "              Text: \"Please review the final report ASAP! We need it ready by our meeting in the morning!\"\n",
        "              Classification: Urgent\n",
        "              Example 3:\n",
        "\n",
        "              Text: \"Let's schedule a meeting for next week to discuss the project. No rush.\"\n",
        "              Classification: Non-Urgent\n",
        "              Example 4:\n",
        "\n",
        "              Text: \"URGENT: The server is down and needs immediate attention!\"\n",
        "              Classification: Urgent\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the following message: {message}\"}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only\n"
      ],
      "metadata": {
        "id": "qYJ9ZnsKVsO3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat(\"Reminder: Tomorrow's team meeting has been postponed. Please update your calendars\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qPxpNEraVsRP",
        "outputId": "c8d2bcd1-d386-42ad-9f1b-be37f3491285"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Classification: Non-Urgent'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### JSON mode and Log Probs"
      ],
      "metadata": {
        "id": "pHnNNONhZoTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT-3.5 model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT-3.5-turbo model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT-3.5 model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo-preview\",\n",
        "        response_format={ \"type\": \"json_object\" },\n",
        "        logprobs=True,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\n",
        "            You are a helpful assistant classifations.\n",
        "\n",
        "            Below are examples of text messages and their classifications. After studying these examples, please classify the new text message at the end.\n",
        "\n",
        "              Example 1:\n",
        "\n",
        "              Text: \"Can you send me the files by tomorrow? It's not urgent, but I'd like to review them.\"\n",
        "              Classification: Non-Urgent\n",
        "              Example 2:\n",
        "\n",
        "              Text: \"Please review the final report ASAP! We need it ready by our meeting in the morning!\"\n",
        "              Classification: Urgent\n",
        "              Example 3:\n",
        "\n",
        "              Text: \"Let's schedule a meeting for next week to discuss the project. No rush.\"\n",
        "              Classification: Non-Urgent\n",
        "              Example 4:\n",
        "\n",
        "              Text: \"URGENT: The server is down and needs immediate attention!\"\n",
        "              Classification: Urgent\n",
        "\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Classify the following message as Non-Urgent or Urgent and return with probability as JSON: {message}\"}\n",
        "        ]\n",
        "    )\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only\n"
      ],
      "metadata": {
        "id": "uV1xvaalaOxw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = chat(\"Reminder: Tomorrow's team meeting has been postponed. Please update your calendars\")"
      ],
      "metadata": {
        "id": "TKx6kntUZtT0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jlgy42XIdEjb",
        "outputId": "1dcf42ab-cf9d-4e82-8284-3d73a3edf072"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"Classification\": \"Non-Urgent\",\n",
            "  \"Probability\": 0.95\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain of thought reasoning"
      ],
      "metadata": {
        "id": "Kmzn4ktYRty4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat(message):\n",
        "    \"\"\"\n",
        "    Send a message to the OpenAI GPT model and return its response.\n",
        "\n",
        "    This function interacts with the OpenAI API, specifically using the GPT model. It takes a user's message as input, sends it to the model, and returns the model's text-only response. The function ensures the AI's output is concise by providing a system-level instruction.\n",
        "\n",
        "    Parameters:\n",
        "    message (str): A string containing the user's message to the AI.\n",
        "\n",
        "    Returns:\n",
        "    str: The text response generated by the GPT model.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4-turbo-preview\",\n",
        "        # response_format={ \"type\": \"json_object\" },\n",
        "        # logprobs=True,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"\"\"\n",
        "            You are a helpful assistant classifations.\n",
        "\n",
        "            Below are examples of questions and how to calculate the answer\n",
        "\n",
        "              Example 1: Arithmetic Problem\n",
        "              Prompt: \"If a shirt costs $20 and there is a 10% discount, how much does the shirt cost after the discount?\"\n",
        "              Chain of Thought Answer:\n",
        "                Calculate the amount of discount: 10% of $20 is $2.\n",
        "                Subtract the discount from the original price: $20 - $2 = $18.\n",
        "                The shirt costs $18 after the discount.\n",
        "\n",
        "              Example 2: Logic Puzzle\n",
        "              Prompt: \"There are four apples and you take away three. How many apples do you have?\"\n",
        "              Chain of Thought Answer:\n",
        "                You start with four apples.\n",
        "                You take away three apples.\n",
        "                After taking three, you now have those three apples.\n",
        "                You have 3 apples\n",
        "\n",
        "              \"\"\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Answer the following question: {message}\"}\n",
        "        ]\n",
        "    )\n",
        "    text_only = response.choices[0].message.content\n",
        "    return text_only\n"
      ],
      "metadata": {
        "id": "bJR8Sy2vRzcS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chat(\"If a tree absorbs 48 pounds of CO2 a year, how much CO2 will 10 trees absorb in a year?\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC9PUbrjSZbZ",
        "outputId": "fbb8c872-d92e-4e10-bd56-5f9df0c639dd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To find the total amount of CO2 absorbed by 10 trees in a year, follow these steps:\n",
            "\n",
            "1. Each tree absorbs 48 pounds of CO2 a year.\n",
            "2. To find out how much 10 trees will absorb, you multiply the amount absorbed by one tree by 10: 48 pounds/tree × 10 trees = 480 pounds.\n",
            "\n",
            "Therefore, 10 trees will absorb 480 pounds of CO2 in a year.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain & summarizing PDF's"
      ],
      "metadata": {
        "id": "p6ti4VrlvFWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a PDF\n",
        "!wget https://arxiv.org/pdf/2401.16212.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE5ZaG8FvOeR",
        "outputId": "130196f0-311f-4878-e79a-219a7130f4a8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-24 06:38:36--  https://arxiv.org/pdf/2401.16212.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 151.101.131.42, 151.101.195.42, 151.101.67.42, ...\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://arxiv.org/pdf/2401.16212 [following]\n",
            "--2024-07-24 06:38:36--  http://arxiv.org/pdf/2401.16212\n",
            "Connecting to arxiv.org (arxiv.org)|151.101.131.42|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 512852 (501K) [application/pdf]\n",
            "Saving to: ‘2401.16212.pdf’\n",
            "\n",
            "\r2401.16212.pdf        0%[                    ]       0  --.-KB/s               \r2401.16212.pdf      100%[===================>] 500.83K  --.-KB/s    in 0.06s   \n",
            "\n",
            "2024-07-24 06:38:36 (8.87 MB/s) - ‘2401.16212.pdf’ saved [512852/512852]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "54LWtNYvvOl5",
        "outputId": "d83712ac-0fec-463d-a2c8-ff8403ae410f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2401.16212.pdf\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://www.morganstanley.com/im/publication/insights/articles/article_increasingreturns.pdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rg3xOtPjDwud",
        "outputId": "4cf63b67-5e12-49d1-ac9c-fab2615115aa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-24 06:39:12--  https://www.morganstanley.com/im/publication/insights/articles/article_increasingreturns.pdf\n",
            "Resolving www.morganstanley.com (www.morganstanley.com)... 23.204.193.233\n",
            "Connecting to www.morganstanley.com (www.morganstanley.com)|23.204.193.233|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/pdf]\n",
            "Saving to: ‘article_increasingreturns.pdf’\n",
            "\n",
            "article_increasingr     [ <=>                ] 570.73K  --.-KB/s    in 0.09s   \n",
            "\n",
            "2024-07-24 06:39:13 (6.05 MB/s) - ‘article_increasingreturns.pdf’ saved [584426]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CcGSNWcdy2X",
        "outputId": "27a8af75-a4a9-4abd-b47a-59a7f8f406c4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2401.16212.pdf\tarticle_increasingreturns.pdf  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-community pypdf langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BO0OSBa0QvLL",
        "outputId": "12069877-9160-42da-ab5c-5a8b1f9b262c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.2.10-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pypdf\n",
            "  Downloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m30.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-openai\n",
            "  Downloading langchain_openai-0.1.17-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (3.9.5)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting langchain<0.3.0,>=0.2.9 (from langchain-community)\n",
            "  Downloading langchain-0.2.11-py3-none-any.whl (990 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.3/990.3 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.3.0,>=0.2.23 (from langchain-community)\n",
            "  Downloading langchain_core-0.2.23-py3-none-any.whl (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.2/374.2 kB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\n",
            "  Downloading langsmith-0.1.93-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (1.37.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain<0.3.0,>=0.2.9->langchain-community)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.9->langchain-community) (2.8.2)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.23->langchain-community)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.23->langchain-community) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.32.0->langchain-openai) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain-community) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.5.15)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.32.0->langchain-openai) (0.14.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.23->langchain-community)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.9->langchain-community) (2.20.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: pypdf, orjson, mypy-extensions, marshmallow, jsonpointer, typing-inspect, tiktoken, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-text-splitters, langchain-openai, langchain, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.11 langchain-community-0.2.10 langchain-core-0.2.23 langchain-openai-0.1.17 langchain-text-splitters-0.2.2 langsmith-0.1.93 marshmallow-3.21.3 mypy-extensions-1.0.0 orjson-3.10.6 pypdf-4.3.1 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader # Update import statement\n",
        "\n",
        "loader = PyPDFLoader(\"2401.16212.pdf\")\n",
        "pages = loader.load_and_split()"
      ],
      "metadata": {
        "id": "34HMl-vPvUlM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pages[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EmGWqFLIvkqV",
        "outputId": "b3d70a8b-0543-4cf3-9669-e093b95b1b59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': '2401.16212.pdf', 'page': 0}, page_content='Better Call GPT, Comparing Large Language Models Against Lawyers\\nLAUREN MARTIN, NICK WHITEHOUSE, STEPHANIE YIU, LIZZIE CATTERSON, RIVINDU\\nPERERA, AI Center of Excellence, Onit Inc., New Zealand\\nThis paper presents a groundbreaking comparison between Large Language Models (LLMs) and traditional legal contract review-\\ners—Junior Lawyers and Legal Process Outsourcers (LPOs). We dissect whether LLMs can outperform humans in accuracy, speed,\\nand cost-efficiency during contract review. Our empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers,\\nuncovering that advanced models match or exceed human accuracy in determining legal issues. In speed, LLMs complete reviews in\\nmere seconds, eclipsing the hours required by their human counterparts. Cost-wise, LLMs operate at a fraction of the price, offering a\\nstaggering 99.97 percent reduction in cost over traditional methods. These results are not just statistics—they signal a seismic shift in\\nlegal practice. LLMs stand poised to disrupt the legal industry, enhancing accessibility and efficiency of legal services. Our research\\nasserts that the era of LLM dominance in legal contract review is upon us, challenging the status quo and calling for a reimagined\\nfuture of legal workflows.\\nCCS Concepts: •Computing methodologies →Natural language generation ;Information extraction ;•Applied computing\\n→Law;Document searching .\\nAdditional Key Words and Phrases: Generative AI, Large Language Models, LegalAI, NLP\\nReference Format:\\nLauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera. 2024. Better Call GPT, Comparing Large Language\\nModels Against Lawyers. 1, 1 (January 2024), 16 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nThe integration of Artificial Intelligence (AI) into the legal sector has opened a new frontier in legal services. However, as\\nof the current state of research, there appears to be a significant gap in exploratory and experimental studies specifically\\naddressing the capabilities of Generative AI and Large Language Models (LLMs) in the context of determination and\\ndiscovery of legal issues. Such studies would be instrumental in understanding how these advanced AI technologies\\nmanage the intricate task of accurately classifying and pinpointing legal matters, a domain traditionally reliant on the\\ndeep, contextual, and specialised knowledge of human legal experts.\\nTo address the identified gap in the research landscape, this study proposes an experimental and exploratory analysis\\nof the performance of LLMs in the legal domain. The research aims to evaluate the capabilities of LLMs contrasting\\ntheir performance against human legal practitioners on high volume real-world legal tasks. These types of high volume\\nlegal tasks are frequently outsourced or pushed to less experienced lawyers, and given the rapid advancements made\\nby LLMs, raises the question of whether LLMs have achieved a level of legal comprehension that is comparable to the\\nquality, accuracy and efficiency of Junior Lawyers or outsourced legal practitioners on such tasks.\\nAuthor’s address: Lauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera, AI Center of Excellence, Onit Inc., Auckland, New\\nZealand, lauren.martin@onit.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\\nof this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on\\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from contact@onit.com.\\n©2023 Copyright held by the owner/author(s). Publication rights licensed to Onit Inc..\\n1arXiv:2401.16212v1  [cs.CY]  24 Jan 2024')"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4-turbo-preview\", api_key=open_ai_key)\n",
        "chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "#using 0:3 because the later pages would have irrelevant content\n",
        "#the model would be able to parse through the text in the tables, but would ignore images\n",
        "res = chain.invoke(pages[0:3])\n",
        "res"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krxv5sE1vij9",
        "outputId": "0b0d2293-c2f1-4b74-9e5f-615f3e408900"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_documents': [Document(metadata={'source': '2401.16212.pdf', 'page': 0}, page_content='Better Call GPT, Comparing Large Language Models Against Lawyers\\nLAUREN MARTIN, NICK WHITEHOUSE, STEPHANIE YIU, LIZZIE CATTERSON, RIVINDU\\nPERERA, AI Center of Excellence, Onit Inc., New Zealand\\nThis paper presents a groundbreaking comparison between Large Language Models (LLMs) and traditional legal contract review-\\ners—Junior Lawyers and Legal Process Outsourcers (LPOs). We dissect whether LLMs can outperform humans in accuracy, speed,\\nand cost-efficiency during contract review. Our empirical analysis benchmarks LLMs against a ground truth set by Senior Lawyers,\\nuncovering that advanced models match or exceed human accuracy in determining legal issues. In speed, LLMs complete reviews in\\nmere seconds, eclipsing the hours required by their human counterparts. Cost-wise, LLMs operate at a fraction of the price, offering a\\nstaggering 99.97 percent reduction in cost over traditional methods. These results are not just statistics—they signal a seismic shift in\\nlegal practice. LLMs stand poised to disrupt the legal industry, enhancing accessibility and efficiency of legal services. Our research\\nasserts that the era of LLM dominance in legal contract review is upon us, challenging the status quo and calling for a reimagined\\nfuture of legal workflows.\\nCCS Concepts: •Computing methodologies →Natural language generation ;Information extraction ;•Applied computing\\n→Law;Document searching .\\nAdditional Key Words and Phrases: Generative AI, Large Language Models, LegalAI, NLP\\nReference Format:\\nLauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera. 2024. Better Call GPT, Comparing Large Language\\nModels Against Lawyers. 1, 1 (January 2024), 16 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1 INTRODUCTION\\nThe integration of Artificial Intelligence (AI) into the legal sector has opened a new frontier in legal services. However, as\\nof the current state of research, there appears to be a significant gap in exploratory and experimental studies specifically\\naddressing the capabilities of Generative AI and Large Language Models (LLMs) in the context of determination and\\ndiscovery of legal issues. Such studies would be instrumental in understanding how these advanced AI technologies\\nmanage the intricate task of accurately classifying and pinpointing legal matters, a domain traditionally reliant on the\\ndeep, contextual, and specialised knowledge of human legal experts.\\nTo address the identified gap in the research landscape, this study proposes an experimental and exploratory analysis\\nof the performance of LLMs in the legal domain. The research aims to evaluate the capabilities of LLMs contrasting\\ntheir performance against human legal practitioners on high volume real-world legal tasks. These types of high volume\\nlegal tasks are frequently outsourced or pushed to less experienced lawyers, and given the rapid advancements made\\nby LLMs, raises the question of whether LLMs have achieved a level of legal comprehension that is comparable to the\\nquality, accuracy and efficiency of Junior Lawyers or outsourced legal practitioners on such tasks.\\nAuthor’s address: Lauren Martin, Nick Whitehouse, Stephanie Yiu, Lizzie Catterson, Rivindu Perera, AI Center of Excellence, Onit Inc., Auckland, New\\nZealand, lauren.martin@onit.com.\\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not\\nmade or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components\\nof this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on\\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from contact@onit.com.\\n©2023 Copyright held by the owner/author(s). Publication rights licensed to Onit Inc..\\n1arXiv:2401.16212v1  [cs.CY]  24 Jan 2024'),\n",
              "  Document(metadata={'source': '2401.16212.pdf', 'page': 1}, page_content='2 Onit AI Center of Excellence\\nSpecifically, we focus on three primary research questions:\\nDo LLMs outperform Junior Lawyers and Legal Process Outsourcers in determination and location of\\nlegal issues in contracts?\\nThis question aims to assess the precision and recall of LLMs compared to human professionals in determining and\\nlocating legal issues.\\nCan LLMs review contracts faster than Junior Lawyers and Legal Process Outsourcers?\\nIn this question, our focus is on evaluating the efficiency (as measured on a temporal perspective) of LLMs in\\nprocessing and responding to legal queries compared to the time taken by human lawyers.\\nCan LLMs review contracts cheaper than Junior Lawyers and Legal Process Outsourcers?\\nThis question focuses on evaluating the comparative costs between Lawyers and LPOs to determine whether LLMs\\nare more cost effective.\\nThrough this research, we aim to contribute a comprehensive understanding of the potential capabilities and\\nlimitations of LLMs in the legal domain, providing valuable insights for Legal and AI practitioners.\\n2 RELATED WORK\\nThe related work in this area encompasses a broad spectrum of technological advancements in generative AI applied on\\nlegal contract reviews and generation of legal contracts. In this section we will be exploring direct and also indirect\\nadvancements towards application of Natural Language Understanding (NLU) and Natural Language Generation (NLG)\\nin legal domain giving the priority to generative AI approaches.\\nGuha et al. [ 6] introduce the LegalBench which is a collaborative benchmark for measuring legal reasoning in LLMs.\\nTheir work focuses on establishing a benchmark for evaluating the capabilities of LLMs in legal reasoning. By employing\\nthe Issue, Rule, Application, Conclusion framework (IRAC), which is a widely used method for organising legal analysis,\\nthe authors have initiated a collaborative project aimed at developing a comprehensive and open-source benchmark.\\nThis endeavour emphasises the potential for computational tools to enhance legal practices, especially in administrative\\nand transactional settings. However, the primary objective of LegalBench is not to replace legal professionals with\\ncomputational systems, but to assess the extent to which current models can support and augment legal reasoning.\\nOur research, in contrast, delves into a more specialised commercial domain within legal technology, focusing on\\nthe application of LLMs in determining and locating legal issues in contracts. This specific focus on contract analysis\\nsets our work apart from the broader legal reasoning perspective adopted in LegalBench. While Guha et al. emphasise\\nthe need for collaborative efforts and in developing benchmarks for legal reasoning with open source data, our study\\nprovides a detailed and practical examination of the capabilities of LLMs compared directly with legal practitioners,\\nincluding Junior Lawyers and LPOs. This aspect of direct comparison is particularly significant as it sheds light on the\\npractical effectiveness and efficiency of LLMs in real-world legal tasks, a perspective not deeply explored in LegalBench.\\nThe significance of our work is further accentuated by its emphasis on the efficiency of LLMs in contract review, a\\ncrucial aspect in legal practice where time is a critical factor. Our study not only benchmarks the performance of LLMs\\nagainst human practitioners but also evaluates their efficiency in terms of time and cost, offering valuable insights into\\nthe practical application of these models in legal settings. This focus on both performance and efficiency provides a\\npragmatic perspective that has an immediate applicability in legal tasks such as contract review, which are common\\nand often time intensive.\\nIn conclusion, while LegalBench establishes a foundational framework for evaluating LLMs in legal reasoning, our\\nresearch offers a more focused and application-oriented analysis within the legal field. By comparing the capabilities'),\n",
              "  Document(metadata={'source': '2401.16212.pdf', 'page': 2}, page_content='Better Call GPT, Comparing Large Language Models Against Lawyers 3\\nand efficiency of LLMs with those of human legal practitioners in the specific context of contract review, our study\\naddresses a critical gap in the current research landscape. This approach not only benchmarks the effectiveness of\\nLLMs in a practical legal task but also provides substantial contributions to the evolving field of legal technology and\\nits applications.\\nTan et al. [ 9] bring a more practical dimension to the theory oriented approach we noticed in the previous research.\\nAlthough this study does not focus on a deep statistical analysis, it provides some important aspect of practical usage of\\nexisting tools. A key aspect of this research, the comparative analysis between ChatGPT and JusticeBot, provides a\\nnuanced understanding of the strengths and limitations of AI-driven legal tools. The study acknowledges that while\\nChatGPT may not always provide perfectly accurate or reliable information, it offers a powerful and intuitive interface\\nfor general public, a significant consideration in enhancing access to legal information.\\nHowever, Tan et al. also identify potential areas for improvement, particularly in the reliability and depth of the legal\\ninformation provided by ChatGPT. In the legal field, where incorrect information can have serious consequences, these\\naspects are of utmost importance. The research suggests the need for continuous improvement and testing of ChatGPT,\\nespecially in the terms of keeping the model updated with the latest legal development and training it on diverse legal\\nscenarios. Although, Open AI claims that ChatGPT currently performs in the level of a legal graduate, there is no\\nevidence in its performance in various legal scenarios. This is one of the key aspect which we tried to address in our\\nresearch.\\nCallister [ 3] delves into the philosophical and cognitive implications of AI in law, addressing concerns about the\\nreliability and consistency of AI, and highlighting potential issues such as information hallucination. It also emphasises\\nthe transformative nature of AI in shifting the traditional paradigms of legal research.\\nWhile Callister [ 3] focus on the philosophical aspect of AI and LLM’s ability to involve in the legal decision making,\\nwe tried to offer a viewpoint of practical capabilities and efficiency of LLM’s in specific legal tasks. We, however, agree\\nwith the cautionary perspective on the broader implications of generative AI for legal epistemology brought to light by\\nCallister [3] which needs to be accounted when using generative AI widely in legal decision making.\\nChoi et al. [ 4] bring a new dimension to the LLMs applicability in legal domain by focusing on ChatGPT’s applicability\\nin legal education. Although this research does not directly align with our study’s interest in AI’s application in the\\nlegal domain, it brings some new perspectives on how LLMs perform in the legal domain. One of the key findings in\\nthis study is the analysis of limited ability of ChaptGPT to perform complex legal analysis and issue spotting which we\\nmainly focused on our research. Our research also extends this dialogue by comparing LLMs’ performance not just\\nto academic standards (as noticed in [ 6]), but also against real world legal practitioners, particularly Junior Lawyers\\nand LPO professionals. This comparison offers a novel perspective on the utility of LLMs in practical legal work, a\\ndimension not fully explored in any of the aforementioned research attempts.\\n3 METHODOLOGY\\nThis section focuses on the research methodology covering how data was collected, analysed, prepared, and how the\\nmodels were selected to be included as part of this research.\\n3.1 Overall Design\\nThe experiment was designed to compare Junior Lawyers and LPOs against LLMs using Senior Lawyers as a benchmark,\\nestablishing ground truth for determining and locating legal issues in contracts. This replicates the process lawyers')],\n",
              " 'output_text': 'The paper \"Better Call GPT, Comparing Large Language Models Against Lawyers\" by Lauren Martin et al. from Onit Inc., New Zealand, presents a pioneering study comparing the performance of Large Language Models (LLMs) with that of human legal professionals (Junior Lawyers and Legal Process Outsourcers) in reviewing legal contracts. The study evaluates the accuracy, speed, and cost-efficiency of LLMs in identifying legal issues within contracts, using Senior Lawyers\\' assessments as a benchmark for accuracy. The findings reveal that LLMs can match or surpass human accuracy, significantly outperform in speed by completing reviews in seconds, and offer a dramatic cost reduction of 99.97% compared to traditional methods. This research suggests a transformative potential for LLMs in the legal industry, indicating a shift towards more accessible and efficient legal services and highlighting the need for a reevaluation of legal workflows in light of these advancements.'}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(res[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yct4RIQ_R6UA",
        "outputId": "1aaf919c-f810-4e21-dfcd-f30a114b3597"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper \"Better Call GPT, Comparing Large Language Models Against Lawyers\" by Lauren Martin et al. from Onit Inc., New Zealand, presents a pioneering study comparing the performance of Large Language Models (LLMs) with that of human legal professionals (Junior Lawyers and Legal Process Outsourcers) in reviewing legal contracts. The study evaluates the accuracy, speed, and cost-efficiency of LLMs in identifying legal issues within contracts, using Senior Lawyers' assessments as a benchmark for accuracy. The findings reveal that LLMs can match or surpass human accuracy, significantly outperform in speed by completing reviews in seconds, and offer a dramatic cost reduction of 99.97% compared to traditional methods. This research suggests a transformative potential for LLMs in the legal industry, indicating a shift towards more accessible and efficient legal services and highlighting the need for a reevaluation of legal workflows in light of these advancements.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature=0.1, model_name=\"gpt-4-turbo-preview\", api_key=open_ai_key)\n",
        "chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "#using 0:3 because the later pages would have irrelevant content\n",
        "#the model would be able to parse through the text in the tables, but would ignore images\n",
        "res = chain.invoke(pages[0:3])\n",
        "res\n",
        "\n",
        "print(res[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mbXG5efmlv9e",
        "outputId": "4368e802-13e7-4f07-f036-c08a2f039213"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The paper \"Better Call GPT, Comparing Large Language Models Against Lawyers\" by Lauren Martin et al. from Onit Inc., New Zealand, presents a groundbreaking study on the efficacy of Large Language Models (LLMs) in legal contract review, comparing their performance with that of human legal professionals, including Junior Lawyers and Legal Process Outsourcers (LPOs), with Senior Lawyers serving as the benchmark. The study finds that LLMs can achieve similar or better accuracy in identifying legal issues, work significantly faster, and offer a dramatic cost reduction of 99.97% over traditional methods. This research suggests a potential shift towards LLM dominance in legal contract review, urging a reevaluation of legal workflows and the future role of legal professionals. It builds on previous work in legal AI, notably the development of LegalBench for measuring legal reasoning and other studies on AI's reliability and application in legal contexts, aiming to provide practical insights into the effectiveness and efficiency of LLMs in legal practices, especially in contract analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.combine_documents.stuff import StuffDocumentsChain\n",
        "from langchain.chains.llm import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Define prompt\n",
        "#use a prompt template to give the model custom commands - how you want the model to behave\n",
        "\n",
        "prompt_template = \"\"\"Write a concise summary in a maximum of 3 bullets of the following text enclosed within three backticks:\n",
        "```{text}```\n",
        "CONCISE SUMMARY:\"\"\"\n",
        "prompt = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "# Define LLM chain\n",
        "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4-turbo-preview\", api_key=open_ai_key)\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "# llm_chain = prompt | llm\n",
        "\n",
        "# Define StuffDocumentsChain\n",
        "stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")\n",
        "\n",
        "res = stuff_chain.invoke(pages[0:3])"
      ],
      "metadata": {
        "id": "csayOsg-wG6l"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(res[\"output_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KEMRXIgRrqh",
        "outputId": "acbc5ec8-30ce-4f43-91c8-aa8e770af5ec"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- The study compares Large Language Models (LLMs) with Junior Lawyers and Legal Process Outsourcers (LPOs) in legal contract review, finding LLMs match or exceed human accuracy, complete reviews significantly faster, and operate at a 99.97% lower cost.\n",
            "- Empirical analysis used Senior Lawyers' assessments as a benchmark for accuracy, demonstrating LLMs' potential to disrupt the legal industry by enhancing the accessibility and efficiency of legal services.\n",
            "- The research highlights a significant shift towards LLM dominance in legal contract review, suggesting a future where legal workflows are significantly transformed by AI technology.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t9zFFrr-SSBk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}